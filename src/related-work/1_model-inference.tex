\section{Model inference}
\label{sec:related:modelinf}

Generally speaking, a model is a representation of a thing that
allows for investigation of its properties. Most of the time, a
model hides the complexity of the item it represents.

In the software engineering field, models help describe software
and systems in order to ease the process of studying them, or
leverage them to build tools, generate documentation, reveal
faults, etc. Such models usually describe the behaviours of the
software being modelled, and can be also known as specifications.
In the Industry, software models are often neglected:
specifications are not up to date or even missing, models are
neither accurate nor sound, and rarely formals.\\
Indeed, writing complete documentation or formal models is often
a tedious and error-prone task. That is why lightweight models
are usually found in the Industry. This leads to several issues,
e.g., the toughness of validating applications with a good test
coverage or the difficulty to diagnose failures, and to maintain
them since they are poorly documented.

A well-known solution to this problem is to infer models. Model
inference is a research field that aims at (automatically)
creating models, expressing functional behaviours of existing
applications.  These models, that help understand how an
application behaves, can be generated from execution traces
(sequences of observed actions)
\cite{Krka:2010:UDE:1810295.1810324}, documentation
\cite{ZhongZXM11}, source code
\cite{Salah05scenariographer,Pradel:2009}, and even network
traces \cite{6079839} or WSDL description
\cite{Bertolino:2009:ASB:1595696.1595719}. Although this area
sounds promising, it still exposes several open problems, which
require further investigation. Among them, the model generation
may lead to a state space explosion problem. Some works construct
lightweight models to avoid this issue \cite{WPX13}, others yield
extrapolated models by merging application's states which, of
course, express more behaviours than those observed
\cite{4023976}.

Model inference is employed for different purposes. One can infer
model from log files in order to retrieve important information
to identify failure causes \cite{4700316}. It has also
successfully been applied to intrusion detection \cite{debar00},
searching for features in execution traces that allow to
distinguish browsers from other programs, using automatically
generated finite automata.
But, from our point of view, the most proeminent uses of model
inference are Model-Based Testing (MBT) and verification, leading
to numerous techniques to automate testing, e.g., with crawlers
\cite{Amalfitano:2012:UGR:2351676.2351717,Joorabchi:2012:REI:2420240.2420457,MobiGUITARIEEESoftware2014}.
One of the key elements of MBT is the model that describes the
behaviour of the system under test (SUT). Such a model is
supposed to provide an abstract view of the SUT, by focusing on
specific aspects, e.g., the change of a system state at runtime.

In literature, we can find different techniques to infer models
that can be organized in two main categories. A few techniques
infer models from source code, i.e. in a white-box context, but
many works focus on a black-box approach to infer models as it is
a more realistic scenario in the Industry. Both categories assume
a set of traces available for learning the models. We refer to
this principle as passive learning. The second category uses
learning algorithms to infer models, enabling the possibility to
actively learn models by interacting with the system.

The paper is organized as follows: Active learning techniques are
presented in subsection \ref{sec:related:modelinf:active}, introducing
$\EuScript{L}^*$-based techniques in subsection
\ref{sec:related:modelinf:active-letoile} and some other incremental learning
techniques in subsection \ref{sec:related:modelinf:active-increment}, plus mentioning
some other techniques in subsection \ref{sec:related:modelinf:active-others}.
Passive learning techniques are described in subsection
\ref{sec:related:modelinf:passive}, covering Finite State Automaton (FSA)
inference techniques in subsection \ref{sec:related:modelinf:passive-fsa},
specification mining in subsection \ref{sec:related:modelinf:passive-spec}, crawling
techniques in subsection \ref{sec:related:modelinf:passive-crawling}, white-box
approaches in subsection \ref{sec:related:modelinf:passive-white}, and a few more
techniques in subsection \ref{sec:related:modelinf:passive-others}.
We discuss the current state of model inference and give
perspectives in subsection \ref{sec:related:modelinf:perspectives}, and we conclude
in subsection \ref{sec:related:modelinf:conclusion}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Active learning}
\label{sec:related:modelinf:active}

Active learning algorithms actively interact with the system to
learn the models \cite{settles.tr09}. Instead of relying on given
traces, these algorithms (a.k.a. learners) ask questions (a.k.a.
queries) to an oracle, e.g., users or generated test cases. A
learner can then use this feedback to improve a model. Moreover,
by asking informative queries (e.g., close to the decision
boundary), an active learner potentially requires much less
examples than a passive learner that learns from data samples. In
the paper, we focus on methods that ask queries to software
systems.

\subsubsection{$\EuScript{L}^*$-based techniques and related}
\label{sec:related:modelinf:active-letoile}

The $\EuScript{L}^*$ algorithm by Angluin \cite{Angluin198787} is
a well-known active learning method that can learn the models of
black box implementations. It uses concept of oracle which
presumably knows the target model and comes up with a
counterexample, if the conjectured model is not correct. By
taking the counterexample into account, the algorithm iterates by
asking new queries and constructing an improbed conjecture, until
we get an automaton that is equivalent to the black box. This
method is interesting but requires a lot of iterations and heavy
use of an expert oracle. It is designed for complete rather
incremental learning.

Raffelt et al. introduced \textit{LearnLib}
\cite{Raffelt:2005:LLA:1081180.1081189}, a library for learning
deterministic finite-state automata. It implements the
$\EuScript{L}^*$ \cite{Angluin198787} learning algorithm for
inferring Deterministic Finite Automata (DFA) and some slight
variants for deriving Mealy machines. Filters can exploit domain
specific properties and, thus, actively reduce the number of
queries that a teacher is asked during a learning phase.
Additionally, statistical data acquisition can be employed to
evaluate the learning procedure. These features make
\textit{LearnLib} a powerful tool if a teacher is available and
DFA are the target model of choice.
Merten et al. revisited \textit{LearnLib} in a tool called
\textit{Next Generation LearnLib} (NGLL) \cite{ngll11}, a
framework providing infrastructure for practical application.

Walkinshaw et al. presented the \textit{QSM} algorithm in
\cite{Walkinshaw07reverseengineering}, an interactive grammar
inference technique to infer the underlying state machine
representation of an existing software system. The approach is
interactive because it generates queries to the user as it
constructs a hypothesis machine, which can be interpreted as
system tests. It is similar to the work done by Hungar in
\cite{hungar}, who uses the $\EuScript{L}^*$ algorithm, but the
\textit{QSM} algorithm presumes that the input sequences offer
some basic coverage of the essential functionality of the system,
in which case the machine can be inferred relatively cheaply by a
process of state merging, compared to the $\EuScript{L}^*$
technique, which systematically and comprehensively explore the
state space of the target machine. It is worth mentioning that
\textit{QSM} does not aim to learn the complete automaton but,
rather, to generalise the supplied traces. It can be supplied a
subset of traces with interesting behaviours, and it will
generalise from these using queries. Tools such as
\textit{Synapse} \cite{LamelaSeijas:2014:SAB:2633448.2633457}
implement the \textit{QSM} algorithm to perform automatic
behaviour inference and implementation comparison for the
programming language Erlang.

More recently, Choi et al. relied on the $\EuScript{L}^*$
algorithm to generate finite state machines \cite{Choi2013}, in
conjunction with a testing approach. They also leverage the
$\EuScript{L}^*$ algorithm to guide the generation of user input
sequences based on the models. The testing engine aims at
interacting with the application under test to discover new
application states, and to build a model accordingly. If an input
sequence contradicts the learned model, the learning algorithm
rebuilds a new model that meets all the previous scenarios.
However, $\EuScript{L}^*$-based testing requires a lot of
expensive restarts and it spends a lot of time in re-exploring
the same execution prefixes. That is why they also proposed a
novel learning-based testing algorithm that avoids restarts and
aggressively merges states in order to quickly prune the state
space.


\subsubsection{Other incremental learning algorithms}
\label{sec:related:modelinf:active-increment}

% TODO:
% - RPNII Dupont
% - IID Parekh (Moore)
% - CGE Meinke (Mealy)

Meinke and Sindhu \cite{tap2011} introduced an incremental
learning algorithm named \textit{IKL} (Incremental Kripke
Learning) for Kripke structures modelling reactive systems.


\subsubsection{Other techniques}
\label{sec:related:modelinf:active-others}

Bertolino et al. presented the \textit{StrawBerry} method in
\cite{Bertolino:2009:ASB:1595696.1595719}, a method that infers a
\textit{Behavior Protocol} automaton from a WSDL description.
WSDL is a format for documenting a variety of Web Services,
containing information about the inputs/outputs and available
operations. \textit{StrawBerry} automatically derives a partial
ordering relation among the invocations of the different WSDL
operations, that is represented as an automaton called
\textit{Behavior Protocol} automaton. This automaton models the
interaction protocol that a client has to follow in order to
correctly interact with the WS. The states of the behavior
protocol automaton are WS execution states and the transitions,
labeled with operation names and input/ouput data, model
possible operation invocations from the client of the WS.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Passive learning}
\label{sec:related:modelinf:passive}

For passive learning techniques, a model is inferred from a
provided set of traces.

\subsubsection{FSA inference techniques}
\label{sec:related:modelinf:passive-fsa}

Most existing approaches for Finite State Automaton (FSA)
inference are based on the \textit{kTail} algorithm
\cite{5009015}. This algorithm generates a FSA from a set of
traces in two steps.  First, it builds a Prefix Tree Acceptor
(PTA), which is a tree where edges are labeled with event names.
The language accepted by the PTA exactly consists of the set of
event sequences recorded in the traces. Then, \textit{kTail}
transforms the PTA into a FSA by merging two states if they share
the same future of length $k$. The future of length $k$ of a
state is defined as the set of the event sequences of maximum
length $k$ that can be accepted from the state. The final
automaton is obtained by merging every pair of states with the
same future of length $k$.

However, FSA inference for complex software systems can produce
imprecise, overgeneralized models containing undesirable
behaviours \cite{4023976}. Lo et al. enhanced the \textit{kTail}
algorithm by introducing a technique that generates precise FSA
using temporal properties inferred from traces to steer the
generation of FSA \cite{Lo:2009:ASB:1595696.1595761}. However,
this approach only considers the observed execution sequences and
does not consider internal state information.

Reiss and Renieris also improved the \textit{kTail} algorithm by
proposing a new algorithm which merges two states if they share
at least one \textit{k-future} \cite{919096}. By using a merging
criterion that is weaker than the \textit{kTail}, their variant
merges more states than \textit{kTail}.
Lo et al. \cite{Lo:2009:ASB:1595696.1595761} also enhanced the
\textit{kTail} algorithm by incorporating state information. They
proposed first inferring temporal properties that generally hold
for the dynamic traces, and then merging states, while ensuring
that the merge does not violate the inferred properties. However,
this approach only considers the observed execution sequences and
does not consider internal state information.

That is why, in \cite{Krka:2010:UDE:1810295.1810324}, Krka et al.
infer object-level behavioral models from dynamically observed
executions, by using not only execution traces but also
dynamically inferred program invariants. An invariant is a
property that holds at a certain point or points in a program.
These are often used in assert statements, documentation, and
formal specifications.

Mariani et al. introduced the \textit{kBehavior} algorithm
\cite{mariani2007dynamic} that incrementally generates a FSA from
a set of traces. When a new trace is submitted to
\textit{kBehavior}, this algorithm first identifies sub-traces of
the input trace that are accepted by sub-automata in the current
automaton (the sub-traces must have a minimal length k, otherwise
they are considered too short to be relevant). Then,
\textit{kBehavior} extends the automaton with the addition of new
branches that suitably connect the identified sub-automata,
producing a new version of the automaton that accepts the entire
input trace. They successfully applied this algorithm to
automatically analyze log files and retrieve important
information to identify failure causes \cite{4700316}, but also
to automatically analyze logs obtained from workloads to retrieve
important information that can relate the failure to its cause
\cite{cotroneo2007investigation}.
Both works describe the \textit{kLFA} technique that generates an
FSA from a set of traces that incorporate information about both
the event sequences and the values of the attributes associated
with events. The generated FSA has special transition labels that
include data-flow symbols.

Lorenzoli et al. also extended \textit{kTail} to produce FSAs
with transitions annotated by algebraic constraints
\cite{Lorenzoli2008}. Their technique, called \textit{gkTail},
generates an EFSA from a set of traces that incorporate
information about both the event sequences and the values of the
parameters associated with event sequences.

Lo et al. evaluated \textit{kTail}, \textit{kBehavior},
\textit{gkTail}, and \textit{kLFA} with a set of 10 case studies
extracted from real software systems in \cite{Lo20122063}. This
empirical comparative study quantifies both the effect of adding
data-flow information within automata and the effectiveness of
the techniques when varying sparseness of traces.


\subsubsection{Specification mining}
\label{sec:related:modelinf:passive-spec}

Ernst et al. proposed automatic deduction of formal
specifications from execution traces
\cite{Ernst:1999:DDL:302405.302467}. Their \textit{Daikon} tool
works by learning likely invariants involving program variables
from dynamic traces. Dynamic invariant detection runs a program,
observes the values that the program computes, and then reports
properties that were true over the observed executions. This is a
machine learning technique that can be applied to arbitrary data.
Daikon's output has been used for generating test cases,
predicting incompatibilities in component integration, automating
theorem proving, repairing inconsistent data structures, and
checking the validity of data streams, among other tasks
\cite{Ernst200735}.

Ammons et al. first introduced the term \textit{specification
mining} \cite{Ammons:2002:MS:565816.503275} to describe a machine
learning technique that infers a specification by observing
program execution and concisely summarizing the frequent
interaction patterns as state machines that capture both temporal
and data dependences. These state machines can be examined by a
programmer, to refine the specification and identify errors, and
can be utilized by automatic verification tools, to find bugs.

Yang et al. \cite{Yang:2006:PMT:1134285.1134325} created
\textit{Perracotta}, an inference engine for temporal API rules
that is able to scale to large programs and work effectively with
the imperfect traces typically available in industrial scenarios,
using an approximate inference algorithm.

Later, Zong et al. \cite{ZhongZXM11} proposed to infer
specifications from API documentation to check whether
implementations match it.  Such specifications do not reflect the
implementation behaviour though. Furthermore, this method can be
applied on condition to have these API documentations in a
readable format.

Taking another direction by leveraging genetic algorithms,
Tonella et al. \cite{TonellaNMLH13} applied a data-clustering
algorithm to execution traces with concrete states in order to
group concrete states into clusters. They then run invariant
inference as in \cite{Ernst:1999:DDL:302405.302467,Ernst200735}
on each cluster to infer a set of invariants for each cluster,
and they iteratively improve the clustering, using a genetic
algorithm, so as to optimize the quality attributes of the
associated FSM model. Each distinct set of invariants produced
for each cluster at the end of the optimization represents an
abstract state and is used as the abstraction function that maps
concrete states to abstract ones. By applying these abstraction
functions to concrete input traces they are able to generate the
output model.


\subsubsection{Crawling techniques through GUIs}
\label{sec:related:modelinf:passive-crawling}

A lot of works, which originate from automatic black-box testing,
retrieve specifications of event-driven applications (desktop
\cite{Memon:2003}, web \cite{webmate12}, or mobile
\cite{Amalfitano:2012:UGR:2351676.2351717,Joorabchi:2012:REI:2420240.2420457,WPX13,MobiGUITARIEEESoftware2014})
by exploring them (a.k.a. crawling). Memon et al. initially
presented GUITAR \cite{Memon:2003}, a tool for scanning desktop
applications which produces event flow graphs and trees showing
the GUI execution behaviours. The generated models are quite
simple and many false event sequences have to be weeded out
later.

Mesbah et al. proposed the tool Crawljax \cite{crawljax:tweb12},
specialised in Ajax applications. It produces a state machine
model to capture the changes of the DOM structures of HTML
documents obtained after triggering events (click, mouseover,
etc.). An interesting feature of Crawljax is the concatenation of
identical states in the model under construction, by comparison
based on the DOM structure. In practice, the model encompasses
all the actions performed by the implementation. To avoid a state
explosion problem, state abstractions should be manually given.
WebMate \cite{webmate12} is another, more recent, model extractor
for web applications.

Crawlers for mobile applications were proposed in
\cite{Amalfitano:2012:UGR:2351676.2351717,Joorabchi:2012:REI:2420240.2420457,MobiGUITARIEEESoftware2014}.
These provide simple trees, depicting the observed GUI. In
\cite{Amalfitano:2012:UGR:2351676.2351717}, paths of the tree not
terminated by a crash detection, are used to re-generate
regression test cases. Amalfitano et al. created
\textit{MobiGUITAR}, an adaptation of the \textit{GUITAR} tool
for automated GUI-driven testing of Android applications.

Yang et al. \cite{WPX13} presented a grey-box testing method for
Android applications whose originality lies in the static
analysis of the code to only infer the events that can be applied
to the GUI. Then, a classical crawling technique is employed to
derive a lightweight models (simple trees). The exploration can
be directed either in breadth-first order or in depth-first
order.


\subsubsection{White-box techniques}
\label{sec:related:modelinf:passive-white}

A few works tried to infer models from source code. For instance,
Salah et al. proposed \textit{Scenariographer}
\cite{Salah05scenariographer}, an algorithm and a tool to
estimate the usage scenarios of a class from its execution
profile. The estimation process produces canonical groups, where
each group comprises a set of similar method invocation sequences
that represent a usage scenario.
In \cite{Pradel:2009}, Pradel and Gross present a scalable
dynamic analysis that infers extremely detailed specifications of
correct method call sequences on multiple related objects.

The methods described in \cite{concolicandroid12,5416728} rely
upon concolic testing to explore symbolic execution paths of the
application and to detect bugs. These white-box approaches
theoretically offer a better code coverage than black-box
automatic testing. However, the number of paths being explored
concretely limits to short paths only, and the constraints have
not to be too complex for being solved.


\subsubsection{Other techniques}
\label{sec:related:modelinf:passive-others}

In \cite{DBLP:conf/soict/DurandS14}, Durand and Salva proposed a
method combining model inference and expert systems to infer
exact yet partial formal models for web applications from log
files, which is not entirely new
\cite{Andrews00broad-spectrumstudies}, except that, to the best
of our knowledge, none of works focused on speed and exactness
for generating models.
Their work has been enhanced in \cite{DBLP:conf/debs/SalvaD15} to
target large production systems of a worldwide tire manufacturer.
While their work is similar to some of the cited works in the
paper, it differs by quickly inferring both exact, rather small,
and formal models of complex systems that can be used in
practice.
